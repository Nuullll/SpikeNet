[Input]

# image size
image_width = 11

image_height = 11

# duration per stimuli (ms)
duration_per_training_image = 100

duration_per_testing_image = 100

# normalized average input firing rate (Hz)
average_firing_rate = 40.0

# categories of MNIST dataset
start_category = 0

end_category = 2


[Network]

# simulation resolutions
dt_ms = 1.

# dt_s = dt_ms / 1000, will be inferred dynamically.

# network size
# input_neuron_number = Input[image_width] * Input[image_height], will be inferred dynamically.
output_neuron_number = 10

dt_s = 0.001

input_neuron_number = 121


[Layer Basics]

# rest and peak potential of output node
o_rest = 0.0

o_peak = 1.0


[LIF Layer]

# rest membrane potential
v_rest = 0.0

# rest threshold
v_th_rest = 1.0

# amount of threshold adjustment each time
dv_th = 0.03

# time constant of LIF neuron, tau = R * C (ms)
tau = 10.0

# refractory period (ms)
refractory = 0

# effective membrane resistance
res = 1.0

# number of winners of lateral inhibition
winners = 4


[Synapse]

# positive learning rate of STDP
learn_rate_p = 5e-4

# scaling factor
learn_rate_p_scaling = 4.

# learn_rate_p_eff = learn_rate_p * learn_rate_p_scaling, will be inferred dynamically.

# negative learning rate of STDP
learn_rate_m = 5e-4

learn_rate_m_scaling = 4.

# learn_rate_m_eff = learn_rate_m * learn_rate_m_scaling, will be inferred dynamically.

# time constant of learning process (ms)
tau_p = 20.0

tau_m = 20.0

# synapse self-decaying
decay = 1e-4

decay_scaling = 1.286

# decay_eff = decay * decay_scaling, will be inferred dynamically.

# strength limit
w_min = 0.2

w_max = 1.0

# initial weight map, available options: 'random', 'min', 'max'
init_weights = random

learn_rate_p_eff = 0.002

learn_rate_m_eff = 0.002

decay_eff = 0.0001286
